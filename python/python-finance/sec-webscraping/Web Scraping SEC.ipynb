{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping the SEC\n",
    "The Securities and Exchange Commission is a regulatory agency that houses numerous financial documents related to public companies. These companies are required to file financial disclosures to the SEC so that investors can quickly evaluate their business performance. In this tutorial, we will explore how to web scrape the SEC using their public database.\n",
    "\n",
    "If you would like more information about the SEC and their data sources, I encourage you to visit the SEC's website directly. Here is the link for your reference: **https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm**\n",
    "\n",
    "Additionally, I am in the process of building a video series on YouTube that covers this topic. If you're interested in watching those videos, please follow this link **https://www.youtube.com/playlist?list=PLcFcktZ0wnNkOo9FQ2wrDcsV0jYqEYu1z**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "We will be using URLs a lot to request the info we need. To make sure this is quick, we will create a function that will allow us to pass through a list of parameters and output a new URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/0001564590-19-011378-index-headers.html'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's first make a function that will make the process of building a url easy.\n",
    "def make_url(base_url , comp):\n",
    "    \n",
    "    url = base_url\n",
    "    \n",
    "    # add each component to the base url\n",
    "    for r in comp:\n",
    "        url = '{}/{}'.format(url, r)\n",
    "        \n",
    "    return url\n",
    "\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/data\"\n",
    "components = ['886982','000156459019011378', '0001564590-19-011378-index-headers.html']\n",
    "make_url(base_url, components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Pulling the documents for a single filing for a single company\n",
    "If we want to pull all the records for a single filing, the process is simple. We pass through the company's CIK number; this will define the company we want to search. Once we do this, we can request the filings for that company.\n",
    "\n",
    "Remember, that when we request the filings, we will get all the filings for that company. If we need to, we can filter the filings to only a specific time range, but this will require that we filter the URLs that only contain those dates. Unfortunately, we will be able just to select a particular year.\n",
    "\n",
    "If you look at the end of the file extension **(0001564590-19-011378)**, the number in the middle is the year of the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/0001564590-19-011408-index-headers.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/0001564590-19-011408-index.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/0001564590-19-011408.txt\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/gs-424b2.htm\n"
     ]
    }
   ],
   "source": [
    "# define a base url, this would be the EDGAR data Archives\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/data\"\n",
    "\n",
    "# define a company to search (GOLDMAN SACHS), this requires a CIK number that is defined by the SEC.\n",
    "cik_num = '886982'\n",
    "\n",
    "# let's get all the filings for Goldman Sachs in a json format.\n",
    "# Alternative is .html & .xml\n",
    "filings_url = make_url(base_url, [cik_num, 'index.json'])\n",
    "\n",
    "# Get the filings and then decode it into a dictionary object.\n",
    "content = requests.get(filings_url)\n",
    "decoded_content = content.json()\n",
    "\n",
    "# Get a single filing number, this way we can request all the documents that were submitted.\n",
    "filing_number = decoded_content['directory']['item'][0]['name']\n",
    "\n",
    "# define the filing url, again I want all the data back as JSON.\n",
    "filing_url = make_url(base_url, [cik_num, filing_number, 'index.json'])\n",
    "\n",
    "# Get the documents submitted for that filing.\n",
    "content = requests.get(filing_url)\n",
    "document_content = content.json()\n",
    "\n",
    "# get a document name\n",
    "for document in document_content['directory']['item']:\n",
    "    if document['type'] != 'image2.gif':\n",
    "        document_name = document['name']\n",
    "        filing_url = make_url(base_url, [cik_num, filing_number, document_name])\n",
    "        print(filing_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Pulling all the documents for all the filings for a single company\n",
    "If we want to pull all the records for all the filings, the process is very similar. All we are going to do is loop through all the filings instead of just grabbing one. Remember, that there can be many filings for a single company so you may get back more than you intend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Grabbing filing : 000156459019011408\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/0001564590-19-011408-index-headers.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/0001564590-19-011408-index.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/0001564590-19-011408.txt\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000001.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000002.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000003.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000004.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000005.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000006.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000007.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000008.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000009.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/g3jv1geevabj000010.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011408/gs-424b2.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grabbing filing : 000156459019011393\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/0001564590-19-011393-index-headers.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/0001564590-19-011393-index.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/0001564590-19-011393.txt\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/gs-424b2.htm\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/gydpiivk55i5000001.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/gydpiivk55i5000002.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/gydpiivk55i5000003.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/gydpiivk55i5000004.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011393/gydpiivk55i5000005.jpg\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grabbing filing : 000156459019011378\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/0001564590-19-011378-index-headers.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/0001564590-19-011378-index.html\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/0001564590-19-011378.txt\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000001.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000002.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000003.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000004.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000005.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000006.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000007.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000008.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000009.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gb13n2izxpwl000010.jpg\n",
      "https://www.sec.gov/Archives/edgar/data/886982/000156459019011378/gs-424b2.htm\n"
     ]
    }
   ],
   "source": [
    "# define a base url, this would be the EDGAR data Archives\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/data\"\n",
    "\n",
    "# define a company to search (GOLDMAN SACHS), this requires a CIK number that is defined by the SEC.\n",
    "cik_num = '886982'\n",
    "\n",
    "# let's get all the filings for Goldman Sachs in a json format.\n",
    "# Alternative is .html & .xml\n",
    "filings_url = make_url(base_url, [cik_num, 'index.json'])\n",
    "\n",
    "# Get the filings and then decode it into a dictionary object.\n",
    "content = requests.get(filings_url)\n",
    "decoded_content = content.json()\n",
    "\n",
    "# Get a filing number, this way we can request all the documents that were submitted.\n",
    "# HERE I AM JUST GRABBING THE FIRST THREE FILINGS REMOVE [0:3] to grab all of them.\n",
    "for filing_number in decoded_content['directory']['item'][0:3]:    \n",
    "    \n",
    "    filing_num = filing_number['name']\n",
    "    print('-'*100)\n",
    "    print('Grabbing filing : {}'.format(filing_num))\n",
    "    \n",
    "    # define the filing url, again I want all the data back as JSON.\n",
    "    filing_url = make_url(base_url, [cik_num, filing_num, 'index.json'])\n",
    "\n",
    "    # Get the documents submitted for that filing.\n",
    "    content = requests.get(filing_url)\n",
    "    document_content = content.json()\n",
    "\n",
    "    # get a document name\n",
    "    for document in document_content['directory']['item']:\n",
    "        document_name = document['name']\n",
    "        filing_url = make_url(base_url, [cik_num, filing_num, document_name])\n",
    "        print(filing_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Pulling the daily index filings\n",
    "The Daily-Index endpoint will return all the filings for a given year. Once the year is selected, we define a quarter for all the filings, and then we can grab the associated files for that quarter. Each quarter has the following four indexes available:\n",
    "\n",
    " - **Company** — sorted by company name\n",
    " - **Form** — sorted by form type\n",
    " - **Master** — sorted by CIK number\n",
    " - **XBRL** — list of submissions containing XBRL financial files, sorted by CIK number; these include Voluntary Filer Program submissions\n",
    " \n",
    "The company, form, and master indexes contain the same information sorted differently. For more information, please visit the documentation provided by the SEC. https://www.sec.gov/edgar/searchedgar/accessing-edgar-data.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Pulling url for Quarter: QTR1\n",
      "URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/index.json\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Pulling files\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190102.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190103.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190104.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190107.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190108.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190109.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190110.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190111.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190114.idx\n",
      "File URL Link: https://www.sec.gov/Archives/edgar/daily-index/2019/QTR1/company.20190115.idx\n"
     ]
    }
   ],
   "source": [
    "# define the urls needed to make the request, let's start with all the daily filings\n",
    "base_url = r\"https://www.sec.gov/Archives/edgar/daily-index\"\n",
    "\n",
    "# The daily-index filings, require a year and content type (html, json, or xml).\n",
    "year_url = make_url(base_url, ['2019', 'index.json'])\n",
    "\n",
    "# request the content for 2019, remember that a JSON strucutre will be sent back so we need to decode it.\n",
    "content = requests.get(year_url)\n",
    "decoded_content = content.json()\n",
    "\n",
    "# the structure is almost identical to other json requests we've made. Go to the item list.\n",
    "# AGAIN ONLY GRABBING A SUBSET OF THE FULL DATASET \n",
    "for item in decoded_content['directory']['item'][0:1]:\n",
    "    \n",
    "    # get the name of the folder\n",
    "    print('-'*100)\n",
    "    print('Pulling url for Quarter: {}'.format(item['name']))\n",
    "    \n",
    "    # The daily-index filings, require a year, a quarter and a content type (html, json, or xml).\n",
    "    qtr_url = make_url(base_url, ['2019', item['name'], 'index.json'])\n",
    "    \n",
    "    # print out the url.\n",
    "    print(\"URL Link: \" + qtr_url)\n",
    "    \n",
    "    # Request, the new url and again it will be a JSON structure.\n",
    "    file_content = requests.get(qtr_url)\n",
    "    decoded_content = file_content.json()\n",
    "    \n",
    "    print('-'*100)\n",
    "    print('Pulling files')\n",
    "\n",
    "    # for each file in the directory items list, print the file type and file href.\n",
    "    # AGAIN DOING A SUBSET\n",
    "    for file in decoded_content['directory']['item'][0:10]:\n",
    "        \n",
    "        file_url = make_url(base_url, ['2019', item['name'], file['name']])\n",
    "        print(\"File URL Link: \" + file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Parsing the master IDX file\n",
    "Out of all the files to parse, I find the Master.idx file the easiest because it is possible to separate each field by a delimiter. Where the other files do not offer such a delimiter making the process even header than it needs to be.\n",
    "\n",
    "The first thing is to load the information to a text file so that way you don't have to make a second request and not burden the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a url, in this case I'll just take one of the urls up above.\n",
    "file_url = r\"https://www.sec.gov/Archives/edgar/daily-index/2019/QTR2/master.20190401.idx\"\n",
    "\n",
    "# request that new content, this will not be a JSON STRUCTURE!\n",
    "content = requests.get(file_url).content\n",
    "\n",
    "# we can always write the content to a file, so we don't need to request it again.\n",
    "with open('master_20190102.txt', 'wb') as f:\n",
    "     f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1236397',\n",
       "  'BRADBURY DANIEL',\n",
       "  '4',\n",
       "  '20190401',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1236397/0000886744-19-000047.txt'],\n",
       " ['1236458',\n",
       "  'WILLIAMS PAUL S',\n",
       "  '4',\n",
       "  '20190401',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1236458/0001227654-19-000074.txt'],\n",
       " ['1237789',\n",
       "  'BLAIR DONALD W',\n",
       "  '4',\n",
       "  '20190401',\n",
       "  'https://www.sec.gov/Archives/edgar/data/1237789/0001127602-19-013788.txt']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's open it and we will now have a byte stream to play with.\n",
    "with open('master_20190102.txt','rb') as f:\n",
    "     byte_data = f.read()\n",
    "\n",
    "# Now that we loaded the data, we have a byte stream that needs to be decoded and then split by double spaces.\n",
    "data = byte_data.decode(\"utf-8\").split('  ')\n",
    "\n",
    "# We need to remove the headers, so look for the end of the header and grab it's index\n",
    "for index, item in enumerate(data):\n",
    "    if \"ftp://ftp.sec.gov/edgar/\" in item:\n",
    "        start_ind = index\n",
    "\n",
    "# define a new dataset with out the header info.\n",
    "data_format = data[start_ind + 1:]\n",
    "\n",
    "master_data = []\n",
    "\n",
    "# now we need to break the data into sections, this way we can move to the final step of getting each row value.\n",
    "for index, item in enumerate(data_format):\n",
    "    \n",
    "    # if it's the first index, it won't be even so treat it differently\n",
    "    if index == 0:\n",
    "        clean_item_data = item.replace('\\n','|').split('|')\n",
    "        clean_item_data = clean_item_data[8:]\n",
    "    else:\n",
    "        clean_item_data = item.replace('\\n','|').split('|')\n",
    "        \n",
    "    for index, row in enumerate(clean_item_data):\n",
    "        \n",
    "        # when you find the text file.\n",
    "        if '.txt' in row:\n",
    "\n",
    "            # grab the values that belong to that row. It's 4 values before and one after.\n",
    "            mini_list = clean_item_data[(index - 4): index + 1]\n",
    "            \n",
    "            if len(mini_list) != 0:\n",
    "                mini_list[4] = \"https://www.sec.gov/Archives/\" + mini_list[4]\n",
    "                master_data.append(mini_list)\n",
    "                \n",
    "# grab the first three items\n",
    "master_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "An extra step we can take is converting our master list of data into a list of dictionaries, where each dictonary represents a single filing document. This way we can easily iterate over the master list to grab the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each document in the master list.\n",
    "for index, document in enumerate(master_data):\n",
    "    \n",
    "    # create a dictionary for each document in the master list\n",
    "    document_dict = {}\n",
    "    document_dict['cik_number'] = document[0]\n",
    "    document_dict['company_name'] = document[1]\n",
    "    document_dict['form_id'] = document[2]\n",
    "    document_dict['date'] = document[3]\n",
    "    document_dict['file_url'] = document[4]\n",
    "    \n",
    "    master_data[index] = document_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab all the 10-K filings from the dataset. If you would like more info on the different financial forms that we have access to, I encourage you to visit **https://www.sec.gov/forms** for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERAL STEEL HOLDINGS INC\n",
      "https://www.sec.gov/Archives/edgar/data/1239188/0001144204-19-017485.txt\n",
      "COMMONWEALTH INCOME & GROWTH FUND V\n",
      "https://www.sec.gov/Archives/edgar/data/1253347/0001654954-19-003881.txt\n",
      "Joway Health Industries Group Inc\n",
      "https://www.sec.gov/Archives/edgar/data/1263364/0001213900-19-005388.txt\n",
      "TRANSAKT LTD.\n",
      "https://www.sec.gov/Archives/edgar/data/1263872/0001062993-19-001480.txt\n",
      "MONITRONICS INTERNATIONAL INC\n",
      "https://www.sec.gov/Archives/edgar/data/1265107/0001265107-19-000004.txt\n"
     ]
    }
   ],
   "source": [
    "# by being in a dictionary format, it'll be easier to get the items we need.\n",
    "for document_dict in master_data[0:150]:\n",
    "\n",
    "    # if it's a 10-K document pull the url and the name.\n",
    "    if document_dict['form_id'] == '10-K':\n",
    "        print(document_dict['company_name'])\n",
    "        print(document_dict['file_url'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
